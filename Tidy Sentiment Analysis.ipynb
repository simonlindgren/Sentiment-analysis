{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Sentiment Analysis\n",
    "This notebook was posted by Simon Lindgren // [@simonlindgren](http://www.twitter.com/simonlindgren) // [simonlindgren.com](http://simonlindgren.com)\n",
    "\n",
    "This type of sentiment analysis considers the text as a combination of its individual words, and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This is the approach described in the book [Text Mining with R](http://tidytextmining.com) by [Julia Silge](http://juliasilge.com) and [David Robinson](http://varianceexplained.org).\n",
    "\n",
    "Dictionary-based methods like these find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text. These methods do not take into account qualifiers before a word, such as in “no good” or “not true”; a lexicon-based method like this is based on unigrams only.\n",
    "\n",
    "The size of the chunk of text that we use to add up unigram sentiment scores can have an effect on the analysis. A text the size of many paragraphs can often have positive and negative sentiment averaged out to about zero, while sentence-sized or paragraph-sized text often works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment lexicons\n",
    "Sentiment analysis demands that we use a sentiment lexicon, a dictionary of words coded by which sentiment they represent. The `tidytext` package comes with three general purpose sentiment lexicons: [AFINN](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010), [bing](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), and [nrc](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidytext)\n",
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiments(\"afinn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiments(\"bing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiments(\"nrc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read documents\n",
    "The code below is based on what was done in [another notebook](https://github.com/simonlindgren/Tidy-Text-first-steps/blob/master/Tidy%2Btext%2Bfirst%2Bsteps.ipynb). It reads a `csv` file into a tidy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents <- read_csv2(\"tidyraw2.csv\")\n",
    "tidy_documents <- documents %>%\n",
    "    unnest_tokens(word,text)\n",
    "    #unnest_tokens(ngram, text, token = \"ngrams\", n = 2)\n",
    "data(stop_words)\n",
    "tidy_documents <- anti_join(tidy_documents, stop_words, by=\"word\")\n",
    "my_stop_words <- read_csv2(\"swestop.csv\")\n",
    "tidy_documents <- anti_join(tidy_documents, my_stop_words, by=\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataframe\n",
    "tidy_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding sentiments\n",
    "Different sentiments are coded into the `nrc` lexicon. Let's choose 'joy' and read it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrcjoy <- get_sentiments(\"nrc\") %>% \n",
    "  filter(sentiment == \"joy\")\n",
    "nrcjoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the actual sentiment analysis by using the `inner_join` function in `dplyr`. For more about different `join` functions, you can have a look [here](http://www.simonlindgren.com/stuff/2017/4/18/dplyr-joins). The code below asks: What are the most common joy words in posts by Joe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_documents %>%\n",
    "  filter(blogger == \"joe\") %>%\n",
    "  inner_join(nrcjoy) %>%\n",
    "  count(word, sort = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map sentiments throughout texts\n",
    "Now, let's see how sentiment changes throughout texts.\n",
    "\n",
    "For this, we need to sort the dataframe by blogger and date, and to write line numbers per blogger that reflect the chronological sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make linenumbers\n",
    "tidy_documents <- tidy_documents %>% \n",
    "    arrange(blogger, date) %>% \n",
    "    group_by(blogger) %>% \n",
    "    mutate(linenumber = row_number()) %>%\n",
    "    ungroup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we use the `bing` lexicon and `inner_join` to get a sentiment score for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_scores <- tidy_documents %>%\n",
    "    inner_join(get_sentiments(\"bing\"))\n",
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Second, we count how many positive and negative words there are in defined sections of the text. We define an `index` here which keeps track of which 20-line section of text we are counting sentiments for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores <- tidy_documents %>%\n",
    "    inner_join(get_sentiments(\"bing\")) %>%  \n",
    "    count(blogger, index = linenumber %/% 20, sentiment)\n",
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Third, we use the `spread()` function from the `tidyr` package to get the negative and positive sentiments in separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores <- tidy_documents %>%\n",
    "    inner_join(get_sentiments(\"bing\")) %>%  \n",
    "    count(blogger, index = linenumber %/% 20, sentiment) %>%\n",
    "    spread(sentiment, n, fill = 0)\n",
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fourth, finally, we calculate a net sentiment (positive minus negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentiment_scores <- tidy_documents %>%\n",
    "    inner_join(get_sentiments(\"bing\")) %>%  \n",
    "    count(blogger, index = linenumber %/% 20, sentiment) %>%\n",
    "    spread(sentiment, n, fill = 0) %>%\n",
    "    mutate(sentiment = positive - negative)\n",
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the sentiment scores across the \"plot trajectory\" of each set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### library(ggplot2)\n",
    "ggplot(sentiment_scores, aes(index, sentiment, fill = blogger)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~blogger, ncol = 2, scales = \"free_x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_documents %>%\n",
    "    count(word) %>%\n",
    "    with(wordcloud(word,n, max.words = 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we get sentiments from `bing` and tag positive and negative words in `tidy_documents` by doing an `inner_join`. We then find the most common positive and negative words (through `count`).\n",
    "\n",
    "Then, to use the `comparison.cloud()` function in the `wordcloud` package, the dataframe must be turned into a matrix. We do this using the `acast()` function from the `reshape2` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(reshape2)\n",
    "\n",
    "tidy_documents %>%\n",
    "  inner_join(get_sentiments(\"bing\")) %>%\n",
    "  count(word, sentiment, sort = TRUE) %>%\n",
    "  acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n",
    "  comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n",
    "                   max.words = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
